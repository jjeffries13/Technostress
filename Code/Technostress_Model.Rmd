---
title: "Technostress Model"
author: "Jay Jeffries"
date: "5/12/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(readxl)

TechnostressData <- read_excel("TechnostressData.xlsx")

library(metafor)
library(robumeta)
library(clubSandwich)
library(esc)
```

## Robust Variance Estimation Base Model
```{r}
base <- robu(formula = fishers_z ~ 1, data = TechnostressData, 
            modelweights = "CORR", studynum = studyID, 
            var.eff.size = var,
            small = TRUE)
print(base)

base2 <- rma(yi = fishers_z, # effect size
                  vi = var, # variance
                  sei = fishers_SE,
                  test = "t", # use t-tests
                  data = TechnostressData, # define data
                  method = "REML") # estimate variances using REML

summary(base2)
```
|       Fisher's z were computed for each of the Pearson's $r$ correlation coefficients and Odds Ratios drawn from these articles. The base model shows us that the average weighted effect size from all effect sizes drawn from the pool of studies is $r(33.1) = 0.20, p < .001,$ $95\%CI [.15, 25], SE = .023$.

### Testing Nesting Effects

```{r meta regressions metafor}
l3_removed <- rma.mv(yi = fishers_z, 
                     V = var, 
                     data = TechnostressData,
                     random = ~ 1 | studyID/ESID, 
                     test = "t", 
                     method = "REML",
                     sigma2 =  c(0, NA)) # constraining level 3 tau^2 to 0 
summary(l3_removed) # This is equal to fitting a simple random-effects model in which we assume that all effect sizes are independent (which we know they are not) since level 3 is held constant at zero.

MLmodel <- rma.mv(yi = fishers_z, # effect size
                  V = var, # variance
                  random = ~1 | studyID/ESID, # nesting structure, where higher-level variable is                       first and lower-level variable is second (nested)
                  # ~1 indicates the inclusion of a random intercept
                  test = "t", # use t-tests
                  data = TechnostressData, # define data
                  method = "REML") # estimate variances using REML

summary(MLmodel)

# fitstats(MLmodel)
```

|       It makes sense to check if nesting individual effect sizes in studies has improved the model. We fit a model in which the level 3 variance (labeled sigma^2.1), representing between-study heterogeneity, is set to zero. In order to estimate this multilevel model, we use the `metafor` package to estimate the mean $\mu$ and variance of the distribution $\tau^2 + \omega^2$. There were 127 effect sizes involved ($k = 37$) and the method of estimation used for this model is restricted maximum likelihood (REML).

- As the indicator of between-study heterogeneity, $\tau$ = .01133 (labeled "sigma^2.1", where $df = 36$)
- As the indicator of within-study heterogeneity, $\omega$ = .01276 (labeled "sigma^2.2", with $df = 128$)
  - There is greater within-study variation than between-study variation. There is not as much variability in the studies than the samples in which the effect sizes are drawn from. 
  - The overall estimated average effect for the base, three-level model is .2046. 
  
### Measure of Heterogeneity

```{r, I square}
# Computing I^2 statistic since rma.mv does not automatically compute this
W <- diag(1/TechnostressData$var)
X <- model.matrix(MLmodel)
P <- W - W %*% X %*% solve(t(X) %*% W %*% X) %*% t(X) %*% W
100 * sum(MLmodel$sigma2) / (sum(MLmodel$sigma2) + (MLmodel$k-MLmodel$p)/sum(diag(P)))

100 * MLmodel$sigma2 / (sum(MLmodel$sigma2) + (MLmodel$k-MLmodel$p)/sum(diag(P)))
```

|       The summation of the two estimated variance components, $\sigma_1^2$ and $\sigma_2^2$, indicates that the model's $I^2 = 98.47$. This indicates that 98.47% of the total variability is attributed to between-study heterogeneity. 
  - Of this proportion, 43.17% is due to between-study effects where 55.30% is due to within-study variability. The remaining 1.53% can be attributed to sampling variance.
 
```{r, ICC}
round(MLmodel$sigma2[1] / sum(MLmodel$sigma2), 3) # ICC calculation
# MLmodel$sigma2[1] tau^2
# sum(MLmodel$sigma2) tau^2 + omega^2
```
  
|       Speaking to the necessity of the multilevel meta-analysis model, the following equation was used to calculate the intraclass correlation, *ICC*:
$\dfrac {\tau^2}{\tau^2 + \omega^2} = \dfrac {.01214403}{.02770068} = .438$

\newpage

### Model Comparison

|       Comparing the model that ignores the level 3 heterogeneity (l3_removed) and the full multilevel model (mlmodel), the average overall effect has changed. Is the simplified (reduced) model better than the (full) three-level model? I will check via ANOVA to compare models:

```{r ANOVA model comparison}
anova(MLmodel, l3_removed) # ANOVA is used to compare nested models
```

|       The Akaike and Bayesian Information Criterion (AIC & BIC) are lower for the full model that includes three levels. In addition, the likelihood ratio test (LRT) shows that the full model explains significantly more variability than the reduced model, where $\chi^2 = 29.0896, p < .05$, which favors performance of the three-level (full) model. The use of an extra degree of freedom ($df = 3$ rather than $df = 2$) and introduction of another level of nested data to the model is justified. In addition, it is important to maintain the full model, as I must acknowledge that each study can contain, or has produced, more than one sample or effect size. I already know that these effect sizes are not purely independent.

### Subgroup Analysis
 
```{r subgroup analysis model}
modmodel <- rma.mv(yi = fishers_z, #effect size
                  V = var, #variance
                  random = ~1 | studyID/ESID, # nesting structure, where higher-level variable is                       first and lower-level variable is second (nested)
                  # ~1 indicates the inclusion of a random intercept
                  mods = ~ dep, # here we introduce the depression subgroup moderator
                  test = "t", # use t-tests
                  data = TechnostressData, # efine data
                  method = "REML") # estimate variances using REML

summary(modmodel)
```

|       When comparing the subgroups anxiety and depression, I found a non-significant difference between the two subgroups $F(1, 122) = -1.41, p = .16$. 

### Prediction Interval 
```{r prediction interval computation}
PI <- predict(MLmodel)
PI

convert_z2r(PI$pi.lb) # Fishers z to r transformation of lower bound PI
convert_z2r(PI$pi.ub) # Fishers z to r transformation of upper bound PI
```
|       PI range includes a negative ($PI = [-.13, .49]$). This indexes how much the regression coefficients that are derived from this pool of studies vary, and can identify what the predicted effect for a randomly selected participant in the population will be at a 95% effectiveness rate. This suggests that the link between anxiety or depression symptomatology and technology usage may elicit a small negative association for some participants.  

### Outlier Analysis
```{r Outlier Analysis}
# Cooks distance by every ES
Cooks <- cooks.distance(MLmodel)
plot(x = Cooks, type="o", pch=19, xlab="Study ID", ylab="Cook's Distance", xaxt="n")
axis(side=1, at=seq_along(Cooks), labels=as.numeric(names(Cooks)))

# Cooks distance by every study
Cooks1 <- cooks.distance(MLmodel, cluster = TechnostressData$studyID,
reestimate=TRUE, parallel="no", ncpus=1, cl=NULL)
plot(x = Cooks1, type="o", pch=19, xlab="Study ID", ylab="Cook's Distance", xaxt="n")
axis(side=1, at=seq_along(Cooks1), labels=as.numeric(names(Cooks1)))

# DFBETA for every study
b <- dfbetas(MLmodel, cluster = TechnostressData$studyID, reestimate=TRUE, ncpus=1, cl=NULL)
b 
max(b) # When study number 6 is deleted from the data, the observed intercept raises to .43
min(b) # When study number 11 is deleted from the data, the observed intercept lowers to -.55

### hat values
y <- hatvalues(MLmodel, cluster = TechnostressData$studyID)
plot(y, type="o", pch=19, xlab="Study", ylab="Predicted Beta (Hat Matrix Values)", xaxt="n")
axis(side=1, at=seq_along(y), labels=as.numeric(names(y)))
```

|       Leave-one-out diagnostics are calculated by refitting the model *k* times (where *k* is the
number of cases). This was performed at both the effect size and study level above through use of Cook's Distances, which can be interpreted as Mahalanobis distance between the entire set of predicted values once with the *i*th case included and once with the ith case excluded from the model fitting (Viechtbauer, 2010). 

## RVE 
### Sandwich Estimation Correction
```{r clubSandwich RVE estimation}
MLcf <- coef_test(MLmodel, #estimation model above
          cluster = TechnostressData$studyID, #define cluster IDs
          vcov = "CR2") #estimation method (CR2 is best)
MLcf 

conf_int(MLmodel,
         cluster = TechnostressData$studyID, 
         vcov = "CR2")
```

|       Above, robust variance estimated (*RVE*) standard errors were calculated via the via sandwich estimator methods using the `clubSandwich` package to avoid over-specification within the previously modeled multilevel random effects. The *RVE* standard errors drawn from this effort match those found in the original model. 

```{r esc fishers z to Pearsons r transformation}
convert_z2r(.203)
```

|       Using the `esc` package, we can convert the estimated average effect size from a Fishers *z* into a Pearson correlation coefficient *r*. Here, $z = .203$ is equal to a significant, positive assocation $r \approx .200, t(33) = 8.73 < .001, 95\%CI [.154, .245], SE = .0232$. 

\newpage

## Meta-Regressions {#Meta-Regression}

```{r meta-regressions metafor, echo = FALSE, warning = FALSE}
regMLmodel <- rma.mv(yi = fishers_z, # effect size
                  V = var, # variance
                  random = ~1 | studyID/ESID, # nesting structure, where higher-level variable is                       first and lower-level variable is second (nested)
                  # ~1 indicates the inclusion of a random intercept
                  mods = ~ pct_fem + dep + avg_age,
                  test = "t", # use t-tests
                  data = TechnostressData, # define data
                  method = "REML") # estimate variances using REML
summary(regMLmodel)

regMLmodel_1 <- rma.mv(yi = fishers_z, # effect size
                  V = var, # variance
                  random = ~1 | studyID/ESID, # nesting structure, where higher-level variable is                       first and lower-level variable is second (nested)
                  # ~1 indicates the inclusion of a random intercept
                  mods = ~ pct_fem,
                  test = "t", # use t-tests
                  data = TechnostressData, # define data
                  method = "REML") # estimate variances using REML
summary(regMLmodel_1)

regMLmodel_2 <- rma.mv(yi = fishers_z, # effect size
                  V = var, # variance
                  random = ~1 | studyID/ESID, # nesting structure, where higher-level variable is                       first and lower-level variable is second (nested)
                  # ~1 indicates the inclusion of a random intercept
                  mods = ~ dep,
                  test = "t", # use t-tests
                  data = TechnostressData, # define data
                  method = "REML") # estimate variances using REML
summary(regMLmodel_2)

regMLmodel_3 <- rma.mv(yi = fishers_z, # effect size
                  V = var, # variance
                  random = ~1 | studyID/ESID, # nesting structure, where higher-level variable is                       first and lower-level variable is second (nested)
                  # ~1 indicates the inclusion of a random intercept
                  mods = ~ avg_age,
                  test = "t", # use t-tests
                  data = TechnostressData, # define data
                  method = "REML") # estimate variances using REML
summary(regMLmodel_3)
```

```{r Meta-Reg Model Fit}
fitstats(regMLmodel) # This should not be compared to the intercept-only MLmodel above because models are not fitted to the same data (missing data for some moderators)
vif(regMLmodel, digits = 3)

round(100 * (MLmodel$sigma2[1] - regMLmodel$sigma2[1]) / MLmodel$sigma2[1], 2) # Psuedo-R^2

regplot(regMLmodel, mod = "pct_fem", xlab = "Percent Female Participants", refline = 0)
# Bubble plot for pct_female moderator

regplot(regMLmodel, mod = "avg_age", xlab = "Average Age of Participants", refline = 0)
# Bubble plot for avg_age of participants
```
|       The test of moderators within the meta-regression model was found to be significant, where $F(3,100) = 5.151, p < .05$. The model fit statistics cannot be directly compared because the same data were not used for both models. Fewer cases were involved in the meta-regression due to missing data pieces where certain studies did not supply the percent of female and average age of participants.

|       The variance inflation factor (*VIF*) is a measure of multicollinearity (correlation between predictors) that measures how much the variance of the estimated regression coefficients are inflated as compared to when the predictor variables are not linearly related. The below equation to calculate each variable's VIF value estimates the same model coefficient if the other moderator variables in the model had not been included in the model and the corresponding variance.

$\dfrac {Var[b_j]}{Var[b_j^`]} = VIF[b_j]$

<small> Let $b_j$ denote the estimate of the *j*th model coefficient of a particular meta-regression model while $b_j^`$ denotes the same coefficient if the other moderators were not included in the model.</small>

|       Multicollinearity can adversely impact models because it can increase the variance of the regression coefficients, and create less reliable regression models. The numerical VIF (decimal) value tells you what percentage the variance is inflated for each coefficient.
- e.g. $VIF = 1.1$ indicates the variance for that coefficient is 10% bigger than you'd expect.
- Guidelines to interpret the VIF easily: 
  - $VIF = 1$ = Not correlated
  - $1 < VIF < 5$ = Moderately correlated
  - $VIF >=5$ = Highly correlated

|       All three predictors fit into the moderately correlated category, where `pct_fem` and `avg_age` are the variables with the greatest inflation present. Still, these values are not an immediate cause of concern.

|       When including the full set of predictors (`pct_fem`, `avg_age`, and `dep`), the model's psuedo $R^2 = 15.35$, where 15.35% of the variability in the average relationship between technology use and psychological stress can be explained by percent of female participants, the average age of participants, and differentiating between anxiety and depression measures. Because one cannot compute an $R_2$ as one might in OLS regression, the pseudo $R^2 = 15.35$ gives us a rough idea of the predictive power of these moderators.

psuedo $R^2 = \dfrac {\tau^2_a - \tau^2_b}{\tau^2_a}$ 
where $a$ denotes the compact, intercept-only model and $b$ the full meta-regression model. 