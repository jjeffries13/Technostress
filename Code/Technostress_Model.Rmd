---
title: "Technostress Model"
author: "Jay Jeffries"
date: "5/12/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(readxl)

TechnostressData <- read_excel("TechnostressData.xlsx")

library(metafor)
library(robumeta)
library(clubSandwich)
library(esc)
```

## Robust Variance Estimation Base Model
```{r}
base <- robu(formula = fishers_z ~ 1, data = TechnostressData, 
            modelweights = "CORR", studynum = studyID, 
            var.eff.size = var,
            small = TRUE)
print(base)

base2 <- rma(yi = fishers_z, # effect size
                  vi = var, # variance
                  sei = fishers_SE,
                  test = "t", # use t-tests
                  data = TechnostressData, # define data
                  method = "REML") # estimate variances using REML

summary(base2)
```
|       Fisher's z were computed for each of the Pearson's $r$ correlation coefficients and Odds Ratios drawn from these articles. The base model shows us that the average weighted effect size from all effect sizes drawn from the pool of studies is $r(33.1) = 0.20, p < .001,$ $95\%CI [.15, 25], SE = .023$.

### Testing Nesting Effects

```{r meta regressions metafor}
l3_removed <- rma.mv(yi = fishers_z, 
                     V = var, 
                     data = TechnostressData,
                     random = ~ 1 | studyID/ESID, 
                     test = "t", 
                     method = "REML",
                     sigma2 =  c(0, NA)) # constraining level 3 tau^2 to 0 
summary(l3_removed) # This is equal to fitting a simple random-effects model in which we assume that all effect sizes are independent (which we know they are not) since level 3 is held constant at zero.

MLmodel <- rma.mv(yi = fishers_z, # effect size
                  V = var, # variance
                  random = ~1 | studyID/ESID, # nesting structure, where higher-level variable is                       first and lower-level variable is second (nested)
                  # ~1 indicates the inclusion of a random intercept
                  test = "t", # use t-tests
                  data = TechnostressData, # define data
                  method = "REML") # estimate variances using REML

summary(MLmodel)

# fitstats(MLmodel)
```

|       It makes sense to check if nesting individual effect sizes in studies has improved the model. We fit a model in which the level 3 variance (labeled sigma^2.1), representing between-study heterogeneity, is set to zero. In order to estimate this multilevel model, we use the `metafor` package to estimate the mean $\mu$ and variance of the distribution $\tau^2 + \omega^2$. There were 127 effect sizes involved ($k = 37$) and the method of estimation used for this model is restricted maximum likelihood (REML).

- As the indicator of between-study heterogeneity, $\tau$ = .01133 (labeled "sigma^2.1", where $df = 36$)
- As the indicator of within-study heterogeneity, $\omega$ = .01276 (labeled "sigma^2.2", with $df = 128$)
  - There is greater within-study variation than between-study variation. There is not as much variability in the studies than the samples in which the effect sizes are drawn from. 
  - The overall estimated average effect for the base, three-level model is .2046. 
  
### Measure of Heterogeneity

```{r, I square}
# Computing I^2 statistic since rma.mv does not automatically compute this
W <- diag(1/TechnostressData$var)
X <- model.matrix(MLmodel)
P <- W - W %*% X %*% solve(t(X) %*% W %*% X) %*% t(X) %*% W
100 * sum(MLmodel$sigma2) / (sum(MLmodel$sigma2) + (MLmodel$k-MLmodel$p)/sum(diag(P)))

100 * MLmodel$sigma2 / (sum(MLmodel$sigma2) + (MLmodel$k-MLmodel$p)/sum(diag(P)))
```

|       The summation of the two estimated variance components, $\sigma_1^2$ and $\sigma_2^2$, indicates that the model's $I^2 = 98.47$. This indicates that 98.47% of the total variability is attributed to between-study heterogeneity. 
  - Of this proportion, 43.17% is due to between-study effects where 55.30% is due to within-study variability. The remaining 1.53% can be attributed to sampling variance.
 
```{r, ICC}
round(MLmodel$sigma2[1] / sum(MLmodel$sigma2), 3) # ICC calculation
# MLmodel$sigma2[1] tau^2
# sum(MLmodel$sigma2) tau^2 + omega^2
```
  
|       Speaking to the necessity of the multilevel meta-analysis model, the following equation was used to calculate the intraclass correlation, *ICC*:
$\dfrac {\tau^2}{\tau^2 + \omega^2} = \dfrac {.01214403}{.02770068} = .438$

\newpage

### Model Comparison

|       Comparing the model that ignores the level 3 heterogeneity (l3_removed) and the full multilevel model (mlmodel), the average overall effect has changed. Is the simplified (reduced) model better than the (full) three-level model? I will check via ANOVA to compare models:

```{r ANOVA model comparison}
anova(MLmodel, l3_removed) # ANOVA is used to compare nested models
```

|       The Akaike and Bayesian Information Criterion (AIC & BIC) are lower for the full model that includes three levels. In addition, the likelihood ratio test (LRT) shows that the full model explains significantly more variability than the reduced model, where $\chi^2 = 29.0896, p < .05$, which favors performance of the three-level (full) model. The use of an extra degree of freedom ($df = 3$ rather than $df = 2$) and introduction of another level of nested data to the model is justified. In addition, it is important to maintain the full model, as I must acknowledge that each study can contain, or has produced, more than one sample or effect size. I already know that these effect sizes are not purely independent.

### Subgroup Analysis
 
```{r subgroup analysis model}
modmodel <- rma.mv(yi = fishers_z, #effect size
                  V = var, #variance
                  random = ~1 | studyID/ESID, # nesting structure, where higher-level variable is                       first and lower-level variable is second (nested)
                  # ~1 indicates the inclusion of a random intercept
                  mods = ~ dep, # here we introduce the depression subgroup moderator
                  test = "t", # use t-tests
                  data = TechnostressData, # efine data
                  method = "REML") # estimate variances using REML

summary(modmodel)
```

|       When comparing the subgroups anxiety and depression, I found a non-significant difference between the two subgroups $F(1, 122) = -1.41, p = .16$. 

### Prediction Interval 
```{r prediction interval computation}
PI <- predict(MLmodel)
PI

convert_z2r(PI$pi.lb) # Fishers z to r transformation of lower bound PI
convert_z2r(PI$pi.ub) # Fishers z to r transformation of upper bound PI
```
|       PI range includes a negative ($PI = [-.13, .49]$). This indexes how much the regression coefficients that are derived from this pool of studies vary, and can identify what the predicted effect for a randomly selected participant in the population will be at a 95% effectiveness rate. This suggests that the link between anxiety or depression symptomatology and technology usage may elicit a small negative association for some participants.  

### Outlier Analysis
```{r Outlier Analysis}
# Cooks distance by every ES
Cooks <- cooks.distance(MLmodel)
plot(x = Cooks, type="o", pch=19, xlab="Study ID", ylab="Cook's Distance", xaxt="n")
axis(side=1, at=seq_along(Cooks), labels=as.numeric(names(Cooks)))

# Cooks distance by every study
Cooks1 <- cooks.distance(MLmodel, cluster = TechnostressData$studyID,
reestimate=TRUE, parallel="no", ncpus=1, cl=NULL)
plot(x = Cooks1, type="o", pch=19, xlab="Study ID", ylab="Cook's Distance", xaxt="n")
axis(side=1, at=seq_along(Cooks1), labels=as.numeric(names(Cooks1)))

# DFBETA for every study
b <- dfbetas(MLmodel, cluster = TechnostressData$studyID, reestimate=TRUE, ncpus=1, cl=NULL)
b 
max(b) # When study number 6 is deleted from the data, the observed intercept raises to .43
min(b) # When study number 11 is deleted from the data, the observed intercept lowers to -.55

### hat values
y <- hatvalues(MLmodel, cluster = TechnostressData$studyID)
plot(y, type="o", pch=19, xlab="Study", ylab="Predicted Beta (Hat Matrix Values)", xaxt="n")
axis(side=1, at=seq_along(y), labels=as.numeric(names(y)))
```

|       Leave-one-out diagnostics are calculated by refitting the model *k* times (where *k* is the
number of cases). This was performed at both the effect size and study level above through use of Cook's Distances, which can be interpreted as Mahalanobis distance between the entire set of predicted values once with the *i*th case included and once with the ith case excluded from the model fitting (Viechtbauer, 2010). 

## RVE 
### Sandwich Estimation Correction
```{r clubSandwich RVE estimation}
MLcf <- coef_test(MLmodel, #estimation model above
          cluster = TechnostressData$studyID, #define cluster IDs
          vcov = "CR2") #estimation method (CR2 is best)
MLcf 

conf_int(MLmodel,
         cluster = TechnostressData$studyID, 
         vcov = "CR2")
```

|       Above, robust variance estimated (*RVE*) standard errors were calculated via the via sandwich estimator methods using the `clubSandwich` package to avoid over-specification within the previously modeled multilevel random effects. The *RVE* standard errors drawn from this effort match those found in the original model. 

```{r esc fishers z to Pearsons r transformation}
convert_z2r(.203)
```

|       Using the `esc` package, we can convert the estimated average effect size from a Fishers *z* into a Pearson correlation coefficient *r*. Here, $z = .203$ is equal to a significant, positive assocation $r \approx .200, t(33) = 8.73 < .001, 95\%CI [.154, .245], SE = .0232$. 